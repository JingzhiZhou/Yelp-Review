{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from lime) (3.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from lime) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from lime) (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from lime) (4.50.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from lime) (0.23.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from lime) (0.17.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from matplotlib->lime) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from matplotlib->lime) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from matplotlib->lime) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from matplotlib->lime) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from matplotlib->lime) (2.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (0.17.0)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.5)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->lime) (1.12.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py): started\n",
      "  Building wheel for lime (setup.py): finished with status 'done'\n",
      "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283850 sha256=3808de26ddf3ce8305f3f3f5e7550a4d91e43e79f078c737491fa2ee1aaaf1ef\n",
      "  Stored in directory: c:\\users\\fannie\\appdata\\local\\pip\\cache\\wheels\\e6\\a6\\20\\cc1e293fcdb67ede666fed293cb895395e7ecceb4467779546\n",
      "Successfully built lime\n",
      "Installing collected packages: lime\n",
      "Successfully installed lime-0.2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: requests in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.2)\n",
      "Installing collected packages: packaging, huggingface-hub, sacremoses, tokenizers, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 19.1\n",
      "    Uninstalling packaging-19.1:\n",
      "      Successfully uninstalled packaging-19.1\n",
      "Successfully installed huggingface-hub-0.1.2 packaging-21.3 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "liwc-text-analysis 1.0.2 requires packaging==19.1, but you'll have packaging 21.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting example\n",
      "  Downloading example-0.1.0.tar.gz (860 bytes)\n",
      "Requirement already satisfied: six in c:\\users\\fannie\\anaconda3\\lib\\site-packages (from example) (1.12.0)\n",
      "Building wheels for collected packages: example\n",
      "  Building wheel for example (setup.py): started\n",
      "  Building wheel for example (setup.py): finished with status 'done'\n",
      "  Created wheel for example: filename=example-0.1.0-py3-none-any.whl size=1243 sha256=add982510ed6af73fa72f5852e7a55106b4f327a30f2411617286a6c79014ecb\n",
      "  Stored in directory: c:\\users\\fannie\\appdata\\local\\pip\\cache\\wheels\\0f\\24\\d2\\9282c61f558dc58e67735404c49c03d873555dd867ddbc674a\n",
      "Successfully built example\n",
      "Installing collected packages: example\n",
      "Successfully installed example-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install example --use-feature=2020-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing,feature_selection\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample=pd.read_csv('review_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and   characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '',str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conduct sort business company overcharged system wasnt even installed properly asked remove additional component werent even installed properly refused take item shoddy workmanship way overpriced extremely poor customer service'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sample[\"text_clean\"] = review_sample[\"text\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))\n",
    "review_sample.at[5,'text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count (classic BoW)\n",
    "vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,1))\n",
    "\n",
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset\n",
    "dtf_train, dtf_test = model_selection.train_test_split(review_sample, test_size=0.3)\n",
    "## get target\n",
    "y_train = dtf_train[\"useful_level\"].values\n",
    "y_test = dtf_test[\"useful_level\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dtf_train[\"text_clean\"]\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 4845)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cant': 714,\n",
       " 'understand': 4510,\n",
       " 'hot': 2134,\n",
       " 'yoga': 4828,\n",
       " 'studio': 4131,\n",
       " 'carpet': 739,\n",
       " 'good': 1899,\n",
       " 'class': 873,\n",
       " 'got': 1905,\n",
       " 'hear': 2038,\n",
       " 'pumping': 3346,\n",
       " 'liked': 2475,\n",
       " 'posture': 3245,\n",
       " 'sequence': 3773,\n",
       " 'lot': 2537,\n",
       " 'get': 1855,\n",
       " 'mat': 2633,\n",
       " 'point': 3206,\n",
       " 'uncomfortable': 4502,\n",
       " 'soggy': 3958,\n",
       " 'thing': 4318,\n",
       " 'id': 2170,\n",
       " 'go': 1888,\n",
       " 'back': 399,\n",
       " 'switched': 4198,\n",
       " 'hard': 2015,\n",
       " 'wood': 4766,\n",
       " 'floor': 1713,\n",
       " 'could': 1056,\n",
       " 'cleaned': 878,\n",
       " 'home': 2103,\n",
       " 'away': 389,\n",
       " 'staff': 4054,\n",
       " 'personable': 3126,\n",
       " 'nice': 2844,\n",
       " 'wifi': 4731,\n",
       " 'close': 892,\n",
       " 'great': 1925,\n",
       " 'selection': 3756,\n",
       " 'life': 2469,\n",
       " 'black': 527,\n",
       " 'woman': 4758,\n",
       " 'hair': 1986,\n",
       " 'serious': 3777,\n",
       " 'business': 667,\n",
       " 'turn': 4472,\n",
       " 'make': 2577,\n",
       " 'finding': 1675,\n",
       " 'decent': 1198,\n",
       " 'beauty': 466,\n",
       " 'supply': 4167,\n",
       " 'store': 4109,\n",
       " 'kind': 2370,\n",
       " 'big': 510,\n",
       " 'deal': 1191,\n",
       " 'unfortunately': 4521,\n",
       " 'cambridge': 700,\n",
       " 'boston': 575,\n",
       " 'proper': 3322,\n",
       " 'sorely': 3979,\n",
       " 'lacking': 2401,\n",
       " 'scene': 3703,\n",
       " 'section': 3744,\n",
       " 'cv': 1160,\n",
       " 'often': 2932,\n",
       " 'dont': 1352,\n",
       " 'cut': 1153,\n",
       " 'enter': 1485,\n",
       " 'venus': 4596,\n",
       " 'cosmetic': 1048,\n",
       " 'probably': 3297,\n",
       " 'best': 501,\n",
       " 'africanamerican': 181,\n",
       " 'area': 308,\n",
       " 'sell': 3760,\n",
       " 'little': 2497,\n",
       " 'bit': 522,\n",
       " 'everything': 1528,\n",
       " 'almost': 217,\n",
       " 'every': 1522,\n",
       " 'old': 2939,\n",
       " 'school': 3710,\n",
       " 'greasestyling': 1923,\n",
       " 'product': 3302,\n",
       " 'childhood': 827,\n",
       " 'pink': 3166,\n",
       " 'lotion': 2538,\n",
       " 'cocoa': 912,\n",
       " 'butter': 672,\n",
       " 'stick': 4093,\n",
       " 'nasty': 2808,\n",
       " 'green': 1930,\n",
       " 'gel': 1840,\n",
       " 'mom': 2752,\n",
       " 'used': 4561,\n",
       " 'put': 3362,\n",
       " 'girl': 1868,\n",
       " 'professional': 3304,\n",
       " 'salonquality': 3664,\n",
       " 'also': 223,\n",
       " 'course': 1066,\n",
       " 'ton': 4389,\n",
       " 'wig': 4732,\n",
       " 'well': 4702,\n",
       " 'extension': 1573,\n",
       " 'color': 924,\n",
       " 'length': 2459,\n",
       " 'texture': 4294,\n",
       " 'imaginableall': 2191,\n",
       " 'entirely': 1490,\n",
       " 'reasonable': 3436,\n",
       " 'price': 3283,\n",
       " 'youre': 4835,\n",
       " 'looking': 2529,\n",
       " 'braid': 597,\n",
       " 'kinky': 2374,\n",
       " 'twist': 4478,\n",
       " 'individual': 2226,\n",
       " 'pack': 3022,\n",
       " 'sold': 3959,\n",
       " 'particularly': 3057,\n",
       " 'always': 228,\n",
       " '20': 37,\n",
       " 'yes': 4824,\n",
       " 'look': 2527,\n",
       " 'like': 2474,\n",
       " 'theyve': 4314,\n",
       " 'shelf': 3816,\n",
       " 'since': 3875,\n",
       " 'early': 1412,\n",
       " 'ninety': 2853,\n",
       " 'ingredient': 2235,\n",
       " 'item': 2309,\n",
       " 'sketchy': 3895,\n",
       " 'damn': 1171,\n",
       " 'coming': 938,\n",
       " 'doesnt': 1342,\n",
       " 'nostalgic': 2878,\n",
       " 'smile': 3932,\n",
       " 'face': 1588,\n",
       " 'definitely': 1219,\n",
       " 'worth': 4783,\n",
       " 'trek': 4445,\n",
       " 'central': 772,\n",
       " 'squarelong': 4045,\n",
       " 'short': 3839,\n",
       " 'relaxed': 3491,\n",
       " 'natural': 2810,\n",
       " 'carefree': 728,\n",
       " 'wild': 4733,\n",
       " 'fried': 1782,\n",
       " 'dyed': 1404,\n",
       " 'laidtotheside': 2405,\n",
       " 'going': 1893,\n",
       " 'find': 1674,\n",
       " 'wedding': 4690,\n",
       " 'reception': 3444,\n",
       " 'next': 2842,\n",
       " 'saturday': 3684,\n",
       " 'im': 2189,\n",
       " 'hoping': 2124,\n",
       " 'updated': 4549,\n",
       " 'review': 3566,\n",
       " 'one': 2945,\n",
       " 'far': 1616,\n",
       " 'wonderful': 4761,\n",
       " 'accommodating': 126,\n",
       " 'specific': 4010,\n",
       " 'requirement': 3533,\n",
       " 'wanted': 4655,\n",
       " 'table': 4203,\n",
       " 'set': 3785,\n",
       " 'worked': 4771,\n",
       " 'allowing': 215,\n",
       " 'come': 932,\n",
       " 'taste': 4242,\n",
       " 'food': 1733,\n",
       " 'wine': 4740,\n",
       " 'beer': 478,\n",
       " 'decide': 1199,\n",
       " 'menu': 2680,\n",
       " 'awesome': 390,\n",
       " 'tasted': 4244,\n",
       " 'amazing': 230,\n",
       " 'including': 2213,\n",
       " 'pizza': 3171,\n",
       " 'ill': 2186,\n",
       " 'follow': 1729,\n",
       " 'concept': 976,\n",
       " 'flip': 1709,\n",
       " 'fairly': 1597,\n",
       " 'simple': 3873,\n",
       " 'burger': 656,\n",
       " 'side': 3860,\n",
       " 'shake': 3800,\n",
       " 'american': 236,\n",
       " 'classic': 874,\n",
       " 'everyone': 1525,\n",
       " 'identify': 2174,\n",
       " 'creative': 1104,\n",
       " 'consultant': 1008,\n",
       " 'behind': 486,\n",
       " 'local': 2509,\n",
       " 'celebrity': 765,\n",
       " 'chef': 816,\n",
       " 'richard': 3577,\n",
       " 'blais': 529,\n",
       " 'expect': 1553,\n",
       " 'modern': 2747,\n",
       " 'update': 4548,\n",
       " 'located': 2513,\n",
       " 'howell': 2140,\n",
       " 'mill': 2718,\n",
       " 'past': 3066,\n",
       " 'chattahoochee': 795,\n",
       " 'avenue': 382,\n",
       " 'boutique': 587,\n",
       " 'feel': 1644,\n",
       " 'retro': 3560,\n",
       " 'joint': 2334,\n",
       " 'sex': 3794,\n",
       " 'city': 868,\n",
       " 'flair': 1694,\n",
       " 'outside': 2991,\n",
       " 'building': 648,\n",
       " 'large': 2414,\n",
       " 'window': 4739,\n",
       " 'reveal': 3565,\n",
       " 'dimly': 1277,\n",
       " 'lit': 2495,\n",
       " 'dining': 1283,\n",
       " 'space': 3997,\n",
       " 'patron': 3078,\n",
       " 'swathed': 4188,\n",
       " 'flattering': 1700,\n",
       " 'candlelight': 709,\n",
       " 'interior': 2268,\n",
       " 'aesthetic': 175,\n",
       " 'described': 1244,\n",
       " 'postmodern': 3244,\n",
       " 'johnny': 2330,\n",
       " 'rocket': 3607,\n",
       " 'stark': 4065,\n",
       " 'white': 4724,\n",
       " 'predominates': 3261,\n",
       " 'highlighted': 2078,\n",
       " 'flash': 1696,\n",
       " 'red': 3456,\n",
       " 'chrome': 855,\n",
       " 'bar': 424,\n",
       " 'flank': 1695,\n",
       " 'entire': 1489,\n",
       " 'room': 3616,\n",
       " 'opposite': 2960,\n",
       " 'cushy': 1148,\n",
       " 'bench': 496,\n",
       " 'intimate': 2275,\n",
       " 'alcove': 204,\n",
       " 'center': 770,\n",
       " 'two': 4479,\n",
       " 'row': 3628,\n",
       " 'adjoining': 157,\n",
       " 'hightop': 2082,\n",
       " 'create': 1100,\n",
       " 'communal': 946,\n",
       " 'ideal': 2172,\n",
       " 'youll': 4833,\n",
       " 'invariably': 2281,\n",
       " 'tempted': 4274,\n",
       " 'peek': 3097,\n",
       " 'see': 3747,\n",
       " 'many': 2603,\n",
       " 'gourmet': 1908,\n",
       " 'neighbor': 2828,\n",
       " 'settled': 3788,\n",
       " 'rear': 3434,\n",
       " 'restaurant': 3551,\n",
       " 'openair': 2952,\n",
       " 'kitchen': 2376,\n",
       " 'observe': 2908,\n",
       " 'feverishly': 1656,\n",
       " 'churning': 856,\n",
       " 'offering': 2925,\n",
       " 'split': 4031,\n",
       " 'category': 753,\n",
       " 'beef': 475,\n",
       " 'organic': 2970,\n",
       " 'nonbeef': 2863,\n",
       " 'alternative': 225,\n",
       " 'satisfy': 3683,\n",
       " 'vegetarian': 4588,\n",
       " 'shun': 3856,\n",
       " 'meat': 2659,\n",
       " 'anyone': 268,\n",
       " 'hankering': 2009,\n",
       " 'pork': 3228,\n",
       " 'patty': 3080,\n",
       " 'slidersized': 3912,\n",
       " 'served': 3780,\n",
       " 'bun': 653,\n",
       " 'partkaiser': 3059,\n",
       " 'part': 3053,\n",
       " 'hawaiian': 2028,\n",
       " 'sweet': 4192,\n",
       " 'roll': 3611,\n",
       " 'actually': 146,\n",
       " 'theyre': 4313,\n",
       " 'slider': 3911,\n",
       " 'steroid': 4091,\n",
       " 'depending': 1239,\n",
       " 'hungry': 2155,\n",
       " 'might': 2706,\n",
       " 'suffice': 4147,\n",
       " 'recommended': 3453,\n",
       " 'larger': 2415,\n",
       " 'appetite': 290,\n",
       " 'indecisives': 2220,\n",
       " 'friend': 1783,\n",
       " 'among': 240,\n",
       " 'three': 4335,\n",
       " 'sampled': 3670,\n",
       " 'different': 1266,\n",
       " 'butcher': 669,\n",
       " 'jelly': 2325,\n",
       " 'caramelized': 724,\n",
       " 'onion': 2947,\n",
       " 'cabrales': 684,\n",
       " 'blue': 549,\n",
       " 'cheese': 810,\n",
       " 'absolute': 112,\n",
       " 'favorite': 1632,\n",
       " 'added': 150,\n",
       " 'hint': 2086,\n",
       " 'savory': 3692,\n",
       " 'tiny': 4367,\n",
       " 'amount': 242,\n",
       " 'packed': 3023,\n",
       " 'tangy': 4232,\n",
       " 'funky': 1808,\n",
       " 'punch': 3348,\n",
       " 'southern': 3992,\n",
       " 'featuring': 1639,\n",
       " 'wpimento': 4793,\n",
       " 'least': 2444,\n",
       " 'huge': 2146,\n",
       " 'fan': 1610,\n",
       " 'pimento': 3163,\n",
       " 'plus': 3202,\n",
       " 'supercrispy': 4164,\n",
       " 'coating': 907,\n",
       " 'encasing': 1463,\n",
       " 'shredded': 3851,\n",
       " 'roof': 3614,\n",
       " 'mouth': 2770,\n",
       " 'fun': 1805,\n",
       " 'po': 3204,\n",
       " 'boyger': 593,\n",
       " 'shrimp': 3852,\n",
       " 'topped': 4400,\n",
       " 'lettuce': 2463,\n",
       " 'tomato': 4387,\n",
       " 'lemon': 2456,\n",
       " 'slice': 3909,\n",
       " 'bay': 453,\n",
       " 'mayo': 2645,\n",
       " 'delicate': 1222,\n",
       " 'flavor': 1702,\n",
       " 'still': 4096,\n",
       " 'tasty': 4247,\n",
       " 'pleased': 3195,\n",
       " 'wasnt': 4674,\n",
       " 'overly': 3005,\n",
       " 'pureed': 3354,\n",
       " 'eating': 1425,\n",
       " 'mi': 2694,\n",
       " 'interpretation': 2271,\n",
       " 'vietnamese': 4612,\n",
       " 'banh': 420,\n",
       " 'hoagiestyle': 2094,\n",
       " 'sandwich': 3673,\n",
       " 'feature': 1638,\n",
       " 'ground': 1947,\n",
       " 'cilantro': 861,\n",
       " 'pickled': 3150,\n",
       " 'veggie': 4589,\n",
       " 'spicy': 4022,\n",
       " 'exotic': 1552,\n",
       " 'complex': 965,\n",
       " 'becoming': 470,\n",
       " 'popular': 3227,\n",
       " 'creation': 1103,\n",
       " 'ordered': 2967,\n",
       " 'sampling': 3671,\n",
       " 'potato': 3247,\n",
       " 'tot': 4407,\n",
       " 'french': 1775,\n",
       " 'fry': 1795,\n",
       " 'vodka': 4624,\n",
       " 'battered': 451,\n",
       " 'ring': 3589,\n",
       " 'accompanying': 129,\n",
       " 'ketchup': 2357,\n",
       " 'dill': 1273,\n",
       " 'honey': 2110,\n",
       " 'mustard': 2795,\n",
       " 'nothing': 2882,\n",
       " 'terribly': 4288,\n",
       " 'remarkable': 3497,\n",
       " 'either': 1444,\n",
       " 'slightly': 3914,\n",
       " 'disappointing': 1296,\n",
       " 'mushy': 2791,\n",
       " 'crispy': 1116,\n",
       " 'dessert': 1256,\n",
       " 'krispy': 2394,\n",
       " 'kreme': 2392,\n",
       " 'chocolate': 840,\n",
       " 'doughnut': 1360,\n",
       " 'blended': 535,\n",
       " 'vanilla': 4578,\n",
       " 'ice': 2168,\n",
       " 'cream': 1096,\n",
       " 'base': 440,\n",
       " 'marshmallow': 2617,\n",
       " 'given': 1871,\n",
       " 'handtorch': 2004,\n",
       " 'toasting': 4375,\n",
       " 'cooled': 1031,\n",
       " 'liquid': 2488,\n",
       " 'nitrogen': 2854,\n",
       " 'aside': 333,\n",
       " 'sounding': 3986,\n",
       " 'wellcool': 4703,\n",
       " 'thick': 4315,\n",
       " 'creamy': 1099,\n",
       " 'treat': 4441,\n",
       " 'stay': 4076,\n",
       " 'way': 4683,\n",
       " 'last': 2417,\n",
       " 'slurp': 3922,\n",
       " 'bill': 513,\n",
       " 'included': 2210,\n",
       " 'came': 701,\n",
       " 'baby': 397,\n",
       " 'averaging': 384,\n",
       " 'itll': 2311,\n",
       " 'interesting': 2267,\n",
       " 'performs': 3118,\n",
       " 'initial': 2237,\n",
       " 'excitement': 1542,\n",
       " 'wear': 4684,\n",
       " 'reality': 3428,\n",
       " 'tight': 4356,\n",
       " 'economy': 1428,\n",
       " 'settle': 3787,\n",
       " 'cheer': 809,\n",
       " 'wwwbonvivantonlinecom': 4804,\n",
       " 'website': 4688,\n",
       " 'address': 155,\n",
       " 'work': 4770,\n",
       " 'wont': 4764,\n",
       " 'let': 2461,\n",
       " 'sign': 3865,\n",
       " 'place': 3173,\n",
       " 'order': 2966,\n",
       " 'online': 2948,\n",
       " 'log': 2518,\n",
       " 'receive': 3440,\n",
       " 'free': 1771,\n",
       " 'cinnaparts': 863,\n",
       " 'offered': 2924,\n",
       " 'know': 2382,\n",
       " 'whati': 4715,\n",
       " 'shouldnt': 3844,\n",
       " 'ordering': 2968,\n",
       " 'anyway': 272,\n",
       " 'rainy': 3398,\n",
       " 'day': 1187,\n",
       " 'location': 2514,\n",
       " 'ph': 3134,\n",
       " 'busy': 668,\n",
       " 'ftr': 1797,\n",
       " 'really': 3431,\n",
       " 'man': 2584,\n",
       " 'taking': 4221,\n",
       " 'seems': 3753,\n",
       " 'stressed': 4118,\n",
       " 'honour': 2114,\n",
       " 'coupon': 1065,\n",
       " 'asked': 335,\n",
       " 'total': 4408,\n",
       " 'around': 318,\n",
       " '18': 30,\n",
       " 'considered': 999,\n",
       " 'megacheap': 2669,\n",
       " 'canada': 704,\n",
       " 'maybe': 2643,\n",
       " 'thats': 4303,\n",
       " 'artificial': 327,\n",
       " 'ready': 3426,\n",
       " '15': 24,\n",
       " 'minute': 2727,\n",
       " 'try': 4463,\n",
       " 'forget': 1745,\n",
       " 'time': 4360,\n",
       " 'found': 1758,\n",
       " 'unused': 4543,\n",
       " 'unvisited': 4544,\n",
       " 'desk': 1254,\n",
       " 'drawer': 1370,\n",
       " 'certain': 773,\n",
       " 'house': 2137,\n",
       " 'left': 2451,\n",
       " '25': 47,\n",
       " 'month': 2759,\n",
       " 'unnoticed': 4535,\n",
       " 'unattended': 4496,\n",
       " 'mediteranean': 2663,\n",
       " 'complete': 961,\n",
       " 'olive': 2942,\n",
       " 'pepper': 3106,\n",
       " 'feta': 1653,\n",
       " 'cheesea': 811,\n",
       " 'perfectly': 3116,\n",
       " 'tact': 4209,\n",
       " 'bite': 523,\n",
       " 'fresh': 1778,\n",
       " 'tonight': 4393,\n",
       " 'seemed': 3751,\n",
       " 'scream': 3719,\n",
       " 'eat': 1422,\n",
       " 'dare': 1174,\n",
       " 'even': 1516,\n",
       " 'worse': 4781,\n",
       " 'much': 2779,\n",
       " 'horrid': 2129,\n",
       " 'driving': 1384,\n",
       " 'commerials': 941,\n",
       " 'thinking': 4321,\n",
       " 'ugh': 4485,\n",
       " 'better': 504,\n",
       " 'pickup': 3151,\n",
       " 'ive': 2312,\n",
       " 'five': 1689,\n",
       " 'watertown': 4682,\n",
       " 'roxbury': 3629,\n",
       " 'site': 3883,\n",
       " 'purchasing': 3352,\n",
       " 'groupon': 1951,\n",
       " '49': 71,\n",
       " '10': 2,\n",
       " 'new': 2836,\n",
       " 'client': 890,\n",
       " 'havent': 2027,\n",
       " 'recently': 3443,\n",
       " 'groupons': 1952,\n",
       " 'seemingly': 3752,\n",
       " 'customer': 1151,\n",
       " 'door': 1354,\n",
       " 'due': 1399,\n",
       " 'poor': 3221,\n",
       " 'quality': 3366,\n",
       " 'service': 3782,\n",
       " 'opposed': 2959,\n",
       " 'notsoideal': 2887,\n",
       " 'inner': 2243,\n",
       " 'strength': 4117,\n",
       " 'need': 2823,\n",
       " 'exorbitant': 1551,\n",
       " 'high': 2073,\n",
       " 'instructor': 2259,\n",
       " 'facility': 1590,\n",
       " 'say': 3694,\n",
       " 'regular': 3480,\n",
       " 'amenity': 235,\n",
       " 'exceptional': 1538,\n",
       " 'le': 2437,\n",
       " 'expensive': 1557,\n",
       " 'think': 4320,\n",
       " 'consider': 997,\n",
       " 'lowering': 2548,\n",
       " 'across': 142,\n",
       " 'board': 552,\n",
       " '17': 29,\n",
       " 'dropin': 1387,\n",
       " 'crazy': 1094,\n",
       " 'barely': 428,\n",
       " 'buy': 676,\n",
       " 'card': 725,\n",
       " 'six': 3887,\n",
       " 'expiration': 1562,\n",
       " '150': 25,\n",
       " 'note': 2881,\n",
       " 'insanely': 2247,\n",
       " 'concrete': 980,\n",
       " 'underneath': 4508,\n",
       " 'notice': 2883,\n",
       " 'standing': 4060,\n",
       " 'rather': 3416,\n",
       " 'waiting': 4639,\n",
       " 'start': 4066,\n",
       " 'savasana': 3690,\n",
       " 'description': 1246,\n",
       " 'heat': 2043,\n",
       " 'heated': 2044,\n",
       " 'talking': 4228,\n",
       " '100degree': 4,\n",
       " 'line': 2481,\n",
       " 'went': 4707,\n",
       " 'dying': 1405,\n",
       " 'first': 1684,\n",
       " 'towel': 4420,\n",
       " 'water': 4680,\n",
       " 'peak': 3092,\n",
       " '5pm': 81,\n",
       " 'later': 2422,\n",
       " 'usually': 4565,\n",
       " 'unbearable': 4499,\n",
       " 'clothes': 899,\n",
       " 'soaking': 3949,\n",
       " 'wet': 4712,\n",
       " 'leave': 2446,\n",
       " 'taken': 4219,\n",
       " 'started': 4067,\n",
       " 'late': 2420,\n",
       " '1015': 7,\n",
       " 'give': 1870,\n",
       " 'end': 1468,\n",
       " '510': 74,\n",
       " 'balance': 413,\n",
       " 'wish': 4748,\n",
       " 'would': 4786,\n",
       " 'startend': 4068,\n",
       " 'definite': 1218,\n",
       " 'dogma': 1344,\n",
       " 'extremely': 1580,\n",
       " 'challenging': 780,\n",
       " 'soapbox': 3950,\n",
       " 'lecture': 2448,\n",
       " 'borderline': 569,\n",
       " 'scold': 3711,\n",
       " 'power': 3255,\n",
       " 'regularly': 3481,\n",
       " 'want': 4654,\n",
       " 'comment': 939,\n",
       " 'unnecessary': 4534,\n",
       " 'something': 3967,\n",
       " 'today': 4376,\n",
       " 'grateful': 1919,\n",
       " 'moment': 2753,\n",
       " 'approach': 300,\n",
       " 'feeling': 1645,\n",
       " 'gratitude': 1920,\n",
       " 'keep': 2352,\n",
       " 'structure': 4128,\n",
       " 'without': 4752,\n",
       " 'variation': 4580,\n",
       " 'amongst': 241,\n",
       " 'teacher': 4257,\n",
       " 'tell': 4268,\n",
       " 'received': 3441,\n",
       " 'training': 4433,\n",
       " 'following': 1731,\n",
       " 'understanding': 4512,\n",
       " 'force': 1740,\n",
       " 'anything': 270,\n",
       " 'child': 826,\n",
       " 'pose': 3233,\n",
       " 'expectation': 1554,\n",
       " 'gave': 1836,\n",
       " 'star': 4062,\n",
       " 'though': 4330,\n",
       " 'powerhot': 3256,\n",
       " 'highly': 2079,\n",
       " 'knowledgable': 2384,\n",
       " 'clean': 877,\n",
       " 'parking': 3049,\n",
       " 'manageable': 2586,\n",
       " 'various': 4582,\n",
       " 'unbelievably': 4501,\n",
       " 'asian': 332,\n",
       " 'mdma': 2650,\n",
       " 'mixed': 2741,\n",
       " 'taiwanese': 4217,\n",
       " 'brunch': 636,\n",
       " 'fantastic': 1614,\n",
       " 'belly': 491,\n",
       " 'turnip': 4474,\n",
       " 'cake': 692,\n",
       " 'soup': 3987,\n",
       " 'dumpling': 1400,\n",
       " 'noodle': 2872,\n",
       " 'soy': 3994,\n",
       " 'milk': 2714,\n",
       " 'cruller': 1126,\n",
       " 'name': 2802,\n",
       " 'form': 1751,\n",
       " 'weather': 4686,\n",
       " 'attest': 368,\n",
       " 'dinner': 1284,\n",
       " 'lunch': 2553,\n",
       " 'average': 383,\n",
       " 'dish': 1312,\n",
       " 'hence': 2060,\n",
       " 'rating': 3417,\n",
       " 'take': 4218,\n",
       " 'happens': 2012,\n",
       " 'vegetable': 4586,\n",
       " 'sauce': 3685,\n",
       " 'congeal': 990,\n",
       " 'oil': 2935,\n",
       " 'separate': 3770,\n",
       " 'sure': 4174,\n",
       " 'exceptionally': 1539,\n",
       " 'bad': 403,\n",
       " 'eaten': 1423,\n",
       " 'inhouse': 2236,\n",
       " 'wait': 4636,\n",
       " 'super': 4162,\n",
       " 'reviewer': 3568,\n",
       " 'said': 3653,\n",
       " 'nononsense': 2868,\n",
       " 'busiest': 666,\n",
       " 'owner': 3017,\n",
       " 'hand': 1996,\n",
       " 'awful': 392,\n",
       " 'older': 2940,\n",
       " 'son': 3972,\n",
       " 'cash': 745,\n",
       " 'register': 3478,\n",
       " 'rude': 3631,\n",
       " 'never': 2834,\n",
       " 'greets': 1935,\n",
       " 'thank': 4297,\n",
       " 'called': 698,\n",
       " 'road': 3598,\n",
       " 'refused': 3475,\n",
       " 'phone': 3141,\n",
       " 'hour': 2136,\n",
       " 'head': 2031,\n",
       " 'onto': 2950,\n",
       " 'plan': 3178,\n",
       " 'explained': 1565,\n",
       " 'told': 4386,\n",
       " 'arrived': 322,\n",
       " 'paid': 3030,\n",
       " '45': 67,\n",
       " 'min': 2720,\n",
       " 'cancel': 705,\n",
       " 'lied': 2468,\n",
       " 'ended': 1470,\n",
       " 'outright': 2990,\n",
       " 'remorse': 3502,\n",
       " 'yelping': 4823,\n",
       " 'barnies': 434,\n",
       " 'specifically': 4011,\n",
       " 'oviedo': 3014,\n",
       " 'enough': 1483,\n",
       " 'starbucks': 4063,\n",
       " 'world': 4777,\n",
       " 'hate': 2023,\n",
       " 'towards': 4418,\n",
       " 'love': 2541,\n",
       " 'absolutely': 113,\n",
       " 'excited': 1541,\n",
       " 'quick': 3377,\n",
       " 'manager': 2589,\n",
       " 'nicest': 2847,\n",
       " 'people': 3104,\n",
       " 'ever': 1521,\n",
       " 'encountered': 1465,\n",
       " 'coffee': 915,\n",
       " 'shop': 3835,\n",
       " 'atmosphere': 358,\n",
       " 'music': 2792,\n",
       " 'general': 1844,\n",
       " 'relax': 3489,\n",
       " 'hang': 2005,\n",
       " 'pay': 3081,\n",
       " 'newspaper': 2840,\n",
       " 'comic': 936,\n",
       " 'morning': 2761,\n",
       " 'pulitzer': 3342,\n",
       " 'prize': 3294,\n",
       " 'persuade': 3129,\n",
       " 'weekly': 4696,\n",
       " 'column': 927,\n",
       " 'social': 3951,\n",
       " 'manner': 2600,\n",
       " 'tale': 4222,\n",
       " 'coupling': 1064,\n",
       " 'crossword': 1121,\n",
       " 'interested': 2266,\n",
       " 'blame': 530,\n",
       " 'globe': 1882,\n",
       " 'capable': 719,\n",
       " 'journalism': 2337,\n",
       " 'hit': 2091,\n",
       " 'paper': 3041,\n",
       " 'washington': 4672,\n",
       " 'post': 3241,\n",
       " 'york': 4831,\n",
       " 'perhaps': 3119,\n",
       " 'nyt': 2903,\n",
       " 'holding': 2098,\n",
       " 'depth': 1241,\n",
       " 'tower': 4421,\n",
       " 'thingy': 4319,\n",
       " 'top': 4398,\n",
       " 'page': 3028,\n",
       " 'idea': 2171,\n",
       " 'improvement': 2207,\n",
       " 'ask': 334,\n",
       " 'g12': 1818,\n",
       " 'finish': 1679,\n",
       " 'article': 326,\n",
       " 'lazy': 2436,\n",
       " 'small': 3923,\n",
       " 'miss': 2732,\n",
       " 'believe': 490,\n",
       " 'experience': 1558,\n",
       " 'recommend': 3451,\n",
       " 'tapa': 4234,\n",
       " 'massage': 2627,\n",
       " 'wrap': 4794,\n",
       " 'promotion': 3319,\n",
       " 'sometimes': 3969,\n",
       " 'discover': 1301,\n",
       " 'youd': 4832,\n",
       " 'regardless': 3477,\n",
       " 'clear': 885,\n",
       " 'resort': 3544,\n",
       " 'tactic': 4210,\n",
       " 'sadly': 3647,\n",
       " 'saw': 3693,\n",
       " 'abrasive': 108,\n",
       " 'repeatedly': 3517,\n",
       " 'decline': 1205,\n",
       " 'upselling': 4554,\n",
       " 'facial': 1589,\n",
       " 'reenter': 3464,\n",
       " 'loudly': 2540,\n",
       " 'disturb': 1333,\n",
       " 'relaxing': 3492,\n",
       " 'portion': 3230,\n",
       " 'eye': 1581,\n",
       " 'closed': 893,\n",
       " 'againafter': 185,\n",
       " 'already': 221,\n",
       " 'politely': 3217,\n",
       " 'declined': 1206,\n",
       " 'numerous': 2895,\n",
       " 'experienced': 1559,\n",
       " 'worst': 4782,\n",
       " '1star': 36,\n",
       " 'technique': 4265,\n",
       " 'attentiveness': 367,\n",
       " 'attitude': 369,\n",
       " 'special': 4008,\n",
       " 'shower': 3848,\n",
       " 'careful': 729,\n",
       " 'knee': 2378,\n",
       " 'dismissively': 1315,\n",
       " 'nodded': 2858,\n",
       " 'proceeded': 3299,\n",
       " 'scrape': 3717,\n",
       " 'drink': 1376,\n",
       " 'nearly': 2817,\n",
       " 'bottom': 581,\n",
       " 'dismissive': 1314,\n",
       " 'pushy': 3361,\n",
       " 'sloppy': 3920,\n",
       " 'unprofessional': 4540,\n",
       " 'dim': 1276,\n",
       " 'sum': 4155,\n",
       " 'chinese': 835,\n",
       " 'west': 4710,\n",
       " 'arbutus': 307,\n",
       " 'especially': 1502,\n",
       " 'westside': 4711,\n",
       " 'standard': 4059,\n",
       " 'disappointed': 1295,\n",
       " 'cart': 742,\n",
       " 'anticipation': 264,\n",
       " 'wondering': 4762,\n",
       " 'weekday': 4694,\n",
       " 'flowing': 1718,\n",
       " 'easy': 1421,\n",
       " 'hurry': 2158,\n",
       " 'impressed': 2202,\n",
       " 'server': 3781,\n",
       " 'refill': 3467,\n",
       " 'teapot': 4259,\n",
       " 'tally': 4229,\n",
       " 'case': 744,\n",
       " 'bubble': 641,\n",
       " 'queen': 3369,\n",
       " 'recent': 3442,\n",
       " 'visit': 4619,\n",
       " '9pm': 104,\n",
       " 'werent': 4709,\n",
       " 'serve': 3779,\n",
       " 'tea': 4256,\n",
       " 'waffle': 4633,\n",
       " 'delicious': 1223,\n",
       " 'long': 2522,\n",
       " 'second': 3742,\n",
       " 'pm': 3203,\n",
       " 'quickly': 3379,\n",
       " 'waited': 4637,\n",
       " 'finally': 1673,\n",
       " 'forgot': 1749,\n",
       " 'another': 260,\n",
       " 'extra': 1575,\n",
       " 'made': 2565,\n",
       " 'discount': 1299,\n",
       " 'uncooked': 4503,\n",
       " 'reservation': 3539,\n",
       " 'matter': 2638,\n",
       " 'week': 4693,\n",
       " 'beautiful': 464,\n",
       " 'spacious': 3998,\n",
       " 'full': 1801,\n",
       " 'crowded': 1123,\n",
       " 'attention': 365,\n",
       " 'friendly': 1786,\n",
       " 'knowledgeable': 2386,\n",
       " 'mainly': 2573,\n",
       " 'chicago': 822,\n",
       " 'yet': 4826,\n",
       " 'compared': 953,\n",
       " 'wolf': 4757,\n",
       " 'ridge': 3583,\n",
       " 'hold': 2096,\n",
       " 'mears': 2657,\n",
       " 'call': 697,\n",
       " 'answered': 263,\n",
       " 'writing': 4799,\n",
       " 'calling': 699,\n",
       " 'shuttle': 3858,\n",
       " 'september': 3772,\n",
       " '2015the': 39,\n",
       " 'waiter': 4638,\n",
       " 'girlfriend': 1869,\n",
       " 'strong': 4126,\n",
       " 'hurricane': 2156,\n",
       " 'funny': 1810,\n",
       " 'guy': 1975,\n",
       " 'joes': 2329,\n",
       " 'crab': 1079,\n",
       " 'shack': 3797,\n",
       " 'discriminating': 1304,\n",
       " 'nightclub': 2850,\n",
       " 'witnessed': 4754,\n",
       " 'victim': 4606,\n",
       " 'race': 3390,\n",
       " 'discrimination': 1305,\n",
       " 'bouncer': 586,\n",
       " 'condones': 981,\n",
       " 'behavior': 485,\n",
       " 'otown': 2982,\n",
       " 'marlows': 2613,\n",
       " 'tavern': 4253,\n",
       " 'group': 1949,\n",
       " 'coworkers': 1077,\n",
       " 'party': 3061,\n",
       " 'efficient': 1435,\n",
       " 'appetizer': 291,\n",
       " 'asparagus': 339,\n",
       " 'mushroom': 2789,\n",
       " 'toast': 4373,\n",
       " 'kettle': 2358,\n",
       " 'chip': 836,\n",
       " 'chicken': 824,\n",
       " 'wing': 4741,\n",
       " 'option': 2963,\n",
       " 'filet': 1665,\n",
       " 'mignon': 2707,\n",
       " 'grilled': 1940,\n",
       " 'medium': 2665,\n",
       " 'rare': 3409,\n",
       " 'jumbo': 2345,\n",
       " 'roasted': 3601,\n",
       " 'fingerling': 1678,\n",
       " 'parslied': 3052,\n",
       " 'seared': 3734,\n",
       " 'cauliflower': 756,\n",
       " 'au': 373,\n",
       " 'poivre': 3209,\n",
       " 'enjoyed': 1480,\n",
       " 'tried': 4451,\n",
       " 'blueberry': 550,\n",
       " 'buck': 642,\n",
       " 'consisted': 1001,\n",
       " 'hangar': 2006,\n",
       " 'lime': 2478,\n",
       " 'syrup': 4200,\n",
       " 'barritts': 436,\n",
       " 'ginger': 1864,\n",
       " 'shaved': 3812,\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try below codes with more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = dtf_train[\"useful_level\"]\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 1\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for cat in np.unique(y):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\"  . selected features:\",\n",
    "         len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "    print(\"  . top features:\", \",\".join(\n",
    "dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "## test\n",
    "X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n",
      "Auc: 0.55\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.33      1.00      0.49        27\n",
      "         2.0       0.00      0.00      0.00        21\n",
      "         3.0       0.43      0.12      0.19        25\n",
      "         4.0       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.33        90\n",
      "   macro avg       0.19      0.28      0.17        90\n",
      "weighted avg       0.22      0.33      0.20        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fannie\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "nlp = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dtf_train[\"text_clean\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split() \n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=300,  window=8, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NaN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
    "## padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=100, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from:  cant understand hot yoga studio carpet good class got hear pumping liked posture sequence lot get mat point uncomfortable soggy carpet thing id go back switched hard wood floor could cleaned class | len: 32\n",
      "to:  [ 112  342  388  636 1143  499    3  164   24  343 1144  246 1553 2282\n",
      "   80    9 2283  210  898 1145  499   43  155   30   12 2284  247  899\n",
      "  437   34  900  164    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] | len: 100\n",
      "check:  cant  -- idx in vocabulary --> 112\n",
      "vocabulary:  {'NaN': 1, 'place': 2, 'good': 3, 'one': 4, 'time': 5} ... (padding element, 0)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "## list of text: [\"I like this\", ...]\n",
    "len_txt = len(dtf_train[\"text_clean\"].iloc[i].split())\n",
    "print(\"from: \", dtf_train[\"text_clean\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "## sequence of token ids: [[1, 2, 3], ...]\n",
    "len_tokens = len(X_train[i])\n",
    "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
    "\n",
    "## vocabulary: \n",
    "print(\"check: \", dtf_train[\"text_clean\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[dtf_train[\"text_clean\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dtf_test[\"text_clean\"]\n",
    "\n",
    "## create list of n-grams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
    "                 len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
    "\n",
    "## padding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=100,\n",
    "             padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 100)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 100)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dic[word]: 1919 |idx\n",
      "embeddings[idx]: (300,) |vector\n"
     ]
    }
   ],
   "source": [
    "word = \"data\"\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
