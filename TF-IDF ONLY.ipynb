{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing,feature_selection\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1=pd.read_csv(r'C:\\Users\\11638\\Favorites\\Code\\Project\\Review_project_sentiment_wUser.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>read_ease</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Adj_ratio</th>\n",
       "      <th>review_count</th>\n",
       "      <th>NumElite</th>\n",
       "      <th>average_stars</th>\n",
       "      <th>NumofWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1073206</td>\n",
       "      <td>JdReKgETiiJEDmshrO4TLw</td>\n",
       "      <td>pyarmAnR-i-qookQamqRTA</td>\n",
       "      <td>V2GOReqPvr8qpCC7sWfoTw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Just to let this car company that people DO re...</td>\n",
       "      <td>2014-03-06 12:38:52</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>let car company people read yelp check review ...</td>\n",
       "      <td>85.49</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6229216</td>\n",
       "      <td>zL4se_Ixdcl8kvTOHCS3rg</td>\n",
       "      <td>s16-BUo-orUsELvMu5ocKg</td>\n",
       "      <td>VH0Ib9S3E-dxbQdQC4rffg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Mistral was the worst dining experience I have...</td>\n",
       "      <td>2010-07-22 18:08:01</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>mistral worst dining experience ever life bad ...</td>\n",
       "      <td>79.19</td>\n",
       "      <td>0.029864</td>\n",
       "      <td>0.502499</td>\n",
       "      <td>0.225888</td>\n",
       "      <td>1777</td>\n",
       "      <td>10</td>\n",
       "      <td>3.82</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               review_id                 user_id  \\\n",
       "0     1073206  JdReKgETiiJEDmshrO4TLw  pyarmAnR-i-qookQamqRTA   \n",
       "1     6229216  zL4se_Ixdcl8kvTOHCS3rg  s16-BUo-orUsELvMu5ocKg   \n",
       "\n",
       "              business_id  stars  useful  funny  cool  \\\n",
       "0  V2GOReqPvr8qpCC7sWfoTw    1.0      17      1     0   \n",
       "1  VH0Ib9S3E-dxbQdQC4rffg    1.0      15      6     4   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  Just to let this car company that people DO re...  2014-03-06 12:38:52   \n",
       "1  Mistral was the worst dining experience I have...  2010-07-22 18:08:01   \n",
       "\n",
       "   ...  language                                         text_clean read_ease  \\\n",
       "0  ...        en  let car company people read yelp check review ...     85.49   \n",
       "1  ...        en  mistral worst dining experience ever life bad ...     79.19   \n",
       "\n",
       "   polarity  subjectivity  Adj_ratio  review_count  NumElite  average_stars  \\\n",
       "0 -0.500000      1.000000   0.120000             9         0           3.00   \n",
       "1  0.029864      0.502499   0.225888          1777        10           3.82   \n",
       "\n",
       "   NumofWords  \n",
       "0          56  \n",
       "1         767  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=5000, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=review1[\"text_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(review1[\"useful_level\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset\n",
    "X_train, X_test,y_train,y_test = model_selection.train_test_split(X,Y, test_size=0.1,stratify=Y)# 10% test size\n",
    "## get target\n",
    "#y_train = dtf_train[\"useful_level\"].values\n",
    "#y_test = np.array(dtf_test[\"useful_level\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_valid,y_train1,y_valid = model_selection.train_test_split(X_train,y_train, test_size=0.2,stratify=y_train)# 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = X_train1\n",
    "vectorizer.fit(corpus1)\n",
    "X_train = vectorizer.transform(corpus1)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39734, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9934,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39734,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train1\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 0.95\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0:\n",
      "  . selected features: 1624\n",
      "  . top features: amazing,call,company,food,great,told,atmosphere,review,delicious,said\n",
      " \n",
      "# 1:\n",
      "  . selected features: 599\n",
      "  . top features: great,car,apartment,food,office,call,told,said,dog,doctor\n",
      " \n",
      "# 2:\n",
      "  . selected features: 436\n",
      "  . top features: tattoo,park,ride,food,great,mall,store,grandson,located,pavilion\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for cat in np.unique(y):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\"  . selected features:\",\n",
    "         len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "    print(\"  . top features:\", \",\".join(\n",
    "dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit vecorizor\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "vectorizer.fit(corpus1)\n",
    "X_train1 = vectorizer.transform(corpus1)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39734, 1725)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "#corpus2 = X_valid\n",
    "#X_valid=vectorizer.transform(corpus2)\n",
    "#corpus2 = dtf_test[\"text_clean\"]\n",
    "#X_test= vectorizer.transform(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X test\n",
    "#corpus3 = X_test\n",
    "#X_test = vectorizer.transform(corpus3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeling and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 1.7405664920806885\n"
     ]
    }
   ],
   "source": [
    "## pipeline\n",
    "t0 = time()\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4884576165442025 0.5432324519126608 0.5026260225255963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "\n",
    "print(f1, p, r)\n",
    "# output: 0.555555555556 0.5 0.666666666667\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Auc: 0.72\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.81      0.70      4361\n",
      "           1       0.48      0.52      0.50      3124\n",
      "           2       0.53      0.18      0.27      2449\n",
      "\n",
      "    accuracy                           0.56      9934\n",
      "   macro avg       0.54      0.50      0.49      9934\n",
      "weighted avg       0.55      0.56      0.53      9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Auc: 0.73\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.80      0.70      2423\n",
      "           1       0.48      0.54      0.51      1736\n",
      "           2       0.53      0.17      0.26      1360\n",
      "\n",
      "    accuracy                           0.56      5519\n",
      "   macro avg       0.54      0.50      0.49      5519\n",
      "weighted avg       0.55      0.56      0.53      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losgistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 8.234264850616455\n",
      "0.5756303749786208 0.5848482625042593 0.5756821517208782\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "classifier = LogisticRegression(max_iter = 1000)\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Auc: 0.79\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77      4361\n",
      "           1       0.53      0.52      0.52      3124\n",
      "           2       0.51      0.38      0.43      2449\n",
      "\n",
      "    accuracy                           0.62      9934\n",
      "   macro avg       0.58      0.58      0.58      9934\n",
      "weighted avg       0.61      0.62      0.61      9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Auc: 0.78\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76      2423\n",
      "           1       0.52      0.52      0.52      1736\n",
      "           2       0.51      0.37      0.43      1360\n",
      "\n",
      "    accuracy                           0.62      5519\n",
      "   macro avg       0.58      0.57      0.57      5519\n",
      "weighted avg       0.60      0.62      0.60      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 62.08448386192322\n",
      "0.3331402107201061 0.4882468525879971 0.3904482209239824\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "classifier = KNeighborsClassifier()\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48\n",
      "Auc: 0.66\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.95      0.63      4361\n",
      "           1       0.47      0.10      0.17      3124\n",
      "           2       0.52      0.12      0.19      2449\n",
      "\n",
      "    accuracy                           0.48      9934\n",
      "   macro avg       0.49      0.39      0.33      9934\n",
      "weighted avg       0.48      0.48      0.38      9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47\n",
      "Auc: 0.65\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.94      0.63      2423\n",
      "           1       0.49      0.11      0.18      1736\n",
      "           2       0.48      0.11      0.18      1360\n",
      "\n",
      "    accuracy                           0.47      5519\n",
      "   macro avg       0.48      0.39      0.33      5519\n",
      "weighted avg       0.48      0.47      0.38      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11638\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:19:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Time: 116.71235227584839\n",
      "0.5596240185545185 0.5809730655025646 0.5611584694990669\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "t0 = time()\n",
    "classifier = xgb.XGBClassifier()\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "Auc: 0.78\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.83      0.76      4361\n",
      "           1       0.51      0.53      0.52      3124\n",
      "           2       0.54      0.32      0.40      2449\n",
      "\n",
      "    accuracy                           0.61      9934\n",
      "   macro avg       0.58      0.56      0.56      9934\n",
      "weighted avg       0.60      0.61      0.60      9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11638\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:21:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.61\n",
      "Auc: 0.78\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.82      0.76      2423\n",
      "           1       0.51      0.55      0.53      1736\n",
      "           2       0.53      0.30      0.39      1360\n",
      "\n",
      "    accuracy                           0.61      5519\n",
      "   macro avg       0.58      0.56      0.56      5519\n",
      "weighted avg       0.60      0.61      0.59      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 45.59788680076599\n",
      "0.4651376759477237 0.4651554832892914 0.4652202140899586\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Auc: 0.6\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.64      4361\n",
      "           1       0.41      0.41      0.41      3124\n",
      "           2       0.35      0.34      0.34      2449\n",
      "\n",
      "    accuracy                           0.50      9934\n",
      "   macro avg       0.47      0.47      0.47      9934\n",
      "weighted avg       0.49      0.50      0.50      9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Auc: 0.61\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.64      0.64      2423\n",
      "           1       0.41      0.43      0.42      1736\n",
      "           2       0.36      0.34      0.35      1360\n",
      "\n",
      "    accuracy                           0.50      5519\n",
      "   macro avg       0.47      0.47      0.47      5519\n",
      "weighted avg       0.50      0.50      0.50      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 182.87185645103455\n",
      "0.5084347985086829 0.5805793158118954 0.5299730788880169\n",
      "Accuracy: 0.6\n",
      "Auc: 0.77\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.86      0.75      4361\n",
      "           1       0.50      0.58      0.54      3124\n",
      "           2       0.58      0.15      0.24      2449\n",
      "\n",
      "    accuracy                           0.60      9934\n",
      "   macro avg       0.58      0.53      0.51      9934\n",
      "weighted avg       0.59      0.60      0.56      9934\n",
      "\n",
      "Accuracy: 0.6\n",
      "Auc: 0.77\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.75      2423\n",
      "           1       0.50      0.61      0.55      1736\n",
      "           2       0.60      0.15      0.24      1360\n",
      "\n",
      "    accuracy                           0.60      5519\n",
      "   macro avg       0.59      0.54      0.51      5519\n",
      "weighted avg       0.60      0.60      0.56      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "t0 = time()\n",
    "classifier = RandomForestClassifier()\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "\n",
    "#On Validation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))\n",
    "\n",
    "#On Test\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting å¤ªä¹…äº†åˆ«run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 1319.9150648117065\n",
      "0.573564657688331 0.5970796610450392 0.5741304260581221\n",
      "Accuracy: 0.62\n",
      "Auc: 0.79\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77      4361\n",
      "           1       0.53      0.54      0.53      3124\n",
      "           2       0.56      0.34      0.42      2449\n",
      "\n",
      "    accuracy                           0.62      9934\n",
      "   macro avg       0.60      0.57      0.57      9934\n",
      "weighted avg       0.61      0.62      0.61      9934\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-64716b0c0194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                            (\"classifier\", classifier)])\n\u001b[0;32m     34\u001b[0m \u001b[1;31m## train classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"classifier\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;31m## validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#X_test = dtf_test[\"text_clean\"].values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.TreeBuilder._check_input\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\csr.py\u001b[0m in \u001b[0;36mtocsc\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         csr_tocsc(self.shape[0], self.shape[1],\n\u001b[0m\u001b[0;32m    183\u001b[0m                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "t0 = time()\n",
    "classifier = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1, max_depth=2)\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "\n",
    "#On Validation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))\n",
    "\n",
    "#On Test\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 338.31313920021057\n",
      "0.546439016086731 0.5472760936834123 0.5461861884665087\n",
      "Accuracy: 0.58\n",
      "Auc: 0.74\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73      4361\n",
      "           1       0.48      0.47      0.47      3124\n",
      "           2       0.46      0.43      0.44      2449\n",
      "\n",
      "    accuracy                           0.58      9934\n",
      "   macro avg       0.55      0.55      0.55      9934\n",
      "weighted avg       0.57      0.58      0.58      9934\n",
      "\n",
      "Accuracy: 0.58\n",
      "Auc: 0.74\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72      2423\n",
      "           1       0.48      0.48      0.48      1736\n",
      "           2       0.45      0.44      0.44      1360\n",
      "\n",
      "    accuracy                           0.58      5519\n",
      "   macro avg       0.55      0.55      0.55      5519\n",
      "weighted avg       0.58      0.58      0.58      5519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "t0 = time()\n",
    "classifier = MLPClassifier(random_state=1, max_iter=500)\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_valid)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_valid)\n",
    "print('Training Time:', time() - t0)\n",
    "\n",
    "#On Validation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score( y_valid, predicted, average='macro' )\n",
    "p = precision_score(y_valid, predicted, average='macro')\n",
    "r = recall_score(y_valid, predicted, average='macro')\n",
    "print(f1, p, r)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = metrics.accuracy_score(y_valid, predicted)\n",
    "auc = metrics.roc_auc_score(y_valid, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_valid, predicted))\n",
    "\n",
    "#On Test\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train1, y_train1.astype(int))\n",
    "## validation\n",
    "#X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "#predicted = np.argmax(predicted,axis=1)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
